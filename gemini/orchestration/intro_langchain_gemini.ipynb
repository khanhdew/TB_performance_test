{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_1uzXWPtI1b_"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359697d5"
      },
      "source": [
        "# Getting Started with LangChain ğŸ¦œï¸ğŸ”— + Gemini API in Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Forchestration%2Fintro_langchain_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/orchestration/intro_langchain_gemini.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_langchain_gemini.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b24d7ab7a7a5"
      },
      "source": [
        "| Authors |\n",
        "| --- |\n",
        "| [Rajesh Thallam](https://github.com/RajeshThallam) |\n",
        "| [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11d788b0"
      },
      "source": [
        "### **What is LangChain?**\n",
        "\n",
        "> LangChain is a framework for developing applications powered by large language models (LLMs).\n",
        "\n",
        "**TL;DR** LangChain makes the complicated parts of working & building with language models easier. It helps do this in two ways:\n",
        "\n",
        "1. **Integration** - Bring external data, such as your files, other applications, and API data, to LLMs\n",
        "2. **Agents** - Allows LLMs to interact with its environment via decision making and use LLMs to help decide which action to take next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpilrb5XVT_k"
      },
      "source": [
        "To build effective Generative AI applications, it is key to enable LLMs to interact with external systems. This makes models data-aware and agentic, meaning they can understand, reason, and use data to take action in a meaningful way. The external systems could be public data corpus, private knowledge repositories, databases, applications, APIs, or access to the public internet via Google Search.\n",
        "\n",
        "Here are a few patterns where LLMs can be augmented with other systems:\n",
        "\n",
        "- Convert natural language to SQL, executing the SQL on database, analyze and present the results\n",
        "- Calling an external webhook or API based on the user query\n",
        "- Synthesize outputs from multiple models, or chain the models in a specific order\n",
        "\n",
        "It may look trivial to plumb these calls together and orchestrate them but it becomes a mundane task to write glue code again and again e.g. for every different data connector or a new model. That's where LangChain comes in!\n",
        "\n",
        "![Augmenting LLMs](https://storage.googleapis.com/gweb-cloudblog-publish/images/Patterns_augmenting_LLMs_with_external_syste.max-900x900.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aATSgLjXVVY0"
      },
      "source": [
        "### **Why LangChain?**\n",
        "\n",
        "LangChain's modular implementation of components and common patterns combining these components makes it easier to build complex applications based on LLMs. LangChain enables these models to connect to data sources and systems as agents to take action.\n",
        "\n",
        "1. **Components** are abstractions that works to bring external data, such as your documents, databases, applications,APIs to language models. LangChain makes it easy to swap out abstractions and components necessary to work with LLMs.\n",
        "\n",
        "2. **Agents** enable language models to communicate with its environment, where the model then decides the next action to take. LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
        "\n",
        "Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
        "\n",
        "### LangChain & Vertex AI\n",
        "\n",
        "[Vertex AI Generative AI models](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) â€” Gemini and Embeddings â€” are officially integrated with the [LangChain Python SDK](https://python.langchain.com/en/latest/index.html), making it convenient to build applications using Gemini models with the ease of use and flexibility of LangChain.\n",
        "\n",
        "- [LangChain Google Integrations](https://python.langchain.com/v0.2/docs/integrations/platforms/google/)\n",
        "\n",
        "---\n",
        "\n",
        "_Note: This notebook does not cover all aspects of LangChain. Its contents have been curated to get you to building & impact as quick as possible. For more, please check out [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50b19d12"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "This notebook provides an introductory understanding of [LangChain](https://langchain.com/) components and use cases of LangChain with the Gemini API in Vertex AI.\n",
        "\n",
        "- Introduce LangChain components\n",
        "- Showcase LangChain + Gemini API in Vertex AI - Text, Chat and Embedding\n",
        "- Summarizing a large text\n",
        "- Question/Answering from PDF (retrieval based)\n",
        "- Chain LLMs with Google Search\n",
        "\n",
        "---\n",
        "\n",
        "**References:**\n",
        "\n",
        "- Adapted from [LangChain Cookbook](https://github.com/gkamradt/langchain-tutorials) from [Greg Kamradt](https://twitter.com/GregKamradt)\n",
        "- [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)\n",
        "- [LangChain Python Documentation](https://python.langchain.com/en/latest/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e985f332"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ohPUPez8imvE",
        "outputId": "c9d74168-368e-42f4-e58e-f13f73530688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.0/67.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m900.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install Vertex AI SDK, LangChain and dependencies\n",
        "%pip install --upgrade --quiet google-cloud-aiplatform langchain langchain-core langchain-text-splitters langchain-google-vertexai langchain-community faiss-cpu langchain-chroma pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034fe628"
      },
      "source": [
        "### Authenticating your notebook environment\n",
        "\n",
        "- If you are using **Colab** to run this notebook, run the cell below and continue.\n",
        "- If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6d83f50a3d3f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f33c406cb7c"
      },
      "source": [
        "- If you are running this notebook in a local development environment:\n",
        "  - Install the [Google Cloud SDK](https://cloud.google.com/sdk).\n",
        "  - Obtain authentication credentials. Create local credentials by running the following command and following the oauth2 flow (read more about the command [here](https://cloud.google.com/sdk/gcloud/reference/beta/auth/application-default/login)):\n",
        "\n",
        "    ```bash\n",
        "    gcloud auth application-default login\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a92ac8ea"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7aedf28aa5e"
      },
      "source": [
        "**Colab only:** Run the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4f6ab09a187c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"rag-test-471014\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "import vertexai\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Smo-7TpE4B22",
        "outputId": "52514ed4-304d-42e4-9143-5abd4bf33aed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import (\n",
        "    ConversationChain,\n",
        "    LLMChain,\n",
        "    RetrievalQA,\n",
        "    SimpleSequentialChain,\n",
        ")\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain_google_vertexai import ChatVertexAI, VertexAI, VertexAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dd3a375d3b6"
      },
      "source": [
        "Define LangChain Models using the Gemini API in Vertex AI for Text, Chat and Vertex AI Embeddings for Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eVpPcvsrkzCk",
        "outputId": "11c7ac50-2eb4-4d89-870e-1358bbacaee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ],
      "source": [
        "# LLM model\n",
        "llm = VertexAI(\n",
        "    model_name=\"gemini-2.5-flash\",\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Chat\n",
        "chat = ChatVertexAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Embedding\n",
        "embeddings = VertexAIEmbeddings(\"text-embedding-005\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05bb564d"
      },
      "source": [
        "## LangChain Components\n",
        "\n",
        "Let's take a quick tour of LangChain framework and concepts to be aware of. LangChain offers a variety of modules that can be used to create language model applications. These modules can be combined to create more complex applications, or can be used individually for simpler applications.\n",
        "\n",
        "![LangChain Components](https://storage.googleapis.com/gweb-cloudblog-publish/images/Figure-3-LangChain_Concepts.max-1300x1300.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyGZZEWlZOe0"
      },
      "source": [
        "- **Models** are the building block of LangChain providing an interface to different types of AI models. Large Language Models (LLMs), Chat and Text Embeddings models are supported model types.\n",
        "- **Prompts** refers to the input to the model, which is typically constructed from multiple components. LangChain provides interfaces to construct and work with prompts easily - Prompt Templates, Example Selectors and Output Parsers.\n",
        "- **Memory** provides a construct for storing and retrieving messages during a conversation which can be either short term or long term.\n",
        "- **Indexes** help LLMs interact with documents by providing a way to structure them. LangChain provides Document Loaders to load documents, Text Splitters to split documents into smaller chunks, Vector Stores to store documents as embeddings, and Retrievers to fetch relevant documents.\n",
        "- **Chains** let you combine modular components (or other chains) in a specific order to complete a task.\n",
        "- **Agents** are a powerful construct in LangChain allowing LLMs to communicate with external systems via Tools and observe and decide on the best course of action to complete a given task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZfIhJSMjDLV"
      },
      "source": [
        "## Schema - Nuts and Bolts of working with LLMs\n",
        "\n",
        "### Text\n",
        "\n",
        "Text is the natural language way to interact with LLMs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8e0dc06c",
        "outputId": "be3080c6-f0b4-428d-81c2-f272a20c8f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Saturday.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# You'll be working with simple strings (that'll soon grow in complexity!)\n",
        "my_text = \"What day comes after Friday?\"\n",
        "\n",
        "llm.invoke(my_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f39eb39"
      },
      "source": [
        "### Chat Messages\n",
        "\n",
        "Chat is like text, but specified with a message type (System, Human, AI)\n",
        "\n",
        "- **System** - Helpful context that tells the AI what to do\n",
        "- **Human** - Messages intended to represent the user\n",
        "- **AI** - Messages showing what the AI responded with\n",
        "\n",
        "For more information, see [LangChain Documentation for Chat Models](https://python.langchain.com/docs/modules/model_io/chat).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qhQOijKAt1ta",
        "outputId": "023d78b3-da48-4a65-c234-46c6057dfdb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 9, 'total_token_count': 634, 'prompt_tokens_details': [{'modality': 1, 'token_count': 1}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 9}], 'thoughts_token_count': 624, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -20.459618462456596, 'model_name': 'gemini-2.5-flash'}, id='run--f24bded5-7be6-47c6-8bf4-cc8b00320f12-0', usage_metadata={'input_tokens': 1, 'output_tokens': 9, 'total_tokens': 634, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 624}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "chat.invoke([HumanMessage(content=\"Hello\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "878d6a36",
        "outputId": "6925a8ed-1c5d-481c-dff8-a207c35403e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enjoy a comforting bowl of tomato soup!\n"
          ]
        }
      ],
      "source": [
        "res = chat.invoke(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"\n",
        "        ),\n",
        "        HumanMessage(content=\"I like tomatoes, what should I eat?\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a425aaa"
      },
      "source": [
        "You can also pass more chat history w/ responses from the AI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3bNQxPln7wC0",
        "outputId": "0ef14aac-7623-4f97-b0be-b148555f588a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making a delicious tomato sandwich is all about simple, fresh ingredients! Here's what you'll need for a classic version:\n",
            "\n",
            "1.  **Bread:** 2 slices per sandwich.\n",
            "    *   **Recommended:** Good quality, soft white bread, wheat bread, or even a sourdough or rye. Some prefer it toasted, others prefer it soft.\n",
            "2.  **Tomatoes:** 1-2 medium-sized tomatoes per sandwich.\n",
            "    *   **Recommended:** Ripe, fresh, firm slicing tomatoes (like beefsteak, heirloom, or other large varieties). Slice them about 1/4 to 1/2 inch thick.\n",
            "3.  **Mayonnaise:**\n",
            "    *   **Recommended:** Good quality, full-fat mayonnaise is often considered essential for the classic taste and texture. Don't skimp!\n",
            "4.  **Salt:**\n",
            "    *   **Recommended:** Flaky sea salt or kosher salt for seasoning the tomatoes. This is crucial for bringing out their flavor.\n",
            "5.  **Black Pepper:**\n",
            "    *   **Recommended:** Freshly ground black pepper.\n",
            "\n",
            "**Optional (but highly recommended for extra flavor):**\n",
            "\n",
            "*   **Fresh Basil Leaves:** A few leaves per sandwich, placed between the tomato slices or on top of the mayo.\n",
            "*   **Butter:** Some like to butter the bread in addition to or instead of mayo.\n",
            "\n",
            "The magic of a great tomato sandwich often lies in the quality and freshness of these simple ingredients!\n"
          ]
        }
      ],
      "source": [
        "res = chat.invoke(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"What are the ingredients required for making a tomato sandwich?\"\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66bf9634"
      },
      "source": [
        "### Documents\n",
        "\n",
        "Document in LangChain refers to an unstructured text consisting of `page_content` referring to the content of the data and `metadata` (data describing attributes of page content).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "150e8759",
        "outputId": "83ce9a8f-0592-4704-f5e1-c142148af078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "Document(\n",
        "    page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
        "    metadata={\n",
        "        \"my_document_id\": 234234,\n",
        "        \"my_document_source\": \"The LangChain Papers\",\n",
        "        \"my_document_create_time\": 1680013019,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2b70f23"
      },
      "source": [
        "### Text Embedding Model\n",
        "\n",
        "[Embeddings](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings) are a way of representing dataâ€“almost any kind of data, like text, images, videos, users, music, whateverâ€“as points in space where the locations of those points in space are semantically meaningful. Embeddings transform your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Vectors are often used when comparing two pieces of text together. An [embedding](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) is a relatively low-dimensional space into which you can translate high-dimensional vectors.\n",
        "\n",
        "[LangChain Text Embedding Model](https://python.langchain.com/v0.2/docs/how_to/embed_text) is integrated with [Vertex AI Embedding API for Text](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings).\n",
        "\n",
        "_BTW: Semantic means 'relating to meaning in language or logic.'_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a2c85e7e"
      },
      "outputs": [],
      "source": [
        "text = \"Hi! It's time for the beach\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ddc5a368",
        "outputId": "6ce21c22-d2fe-4423-ce3e-bebf90628086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your embedding is length 768\n",
            "Here's a sample: [-0.02593194507062435, -0.019493747502565384, -0.03560653328895569, -0.02734011597931385, -0.017964690923690796]...\n"
          ]
        }
      ],
      "source": [
        "text_embedding = embeddings.embed_query(text)\n",
        "print(f\"Your embedding is length {len(text_embedding)}\")\n",
        "print(f\"Here's a sample: {text_embedding[:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c38fe99f"
      },
      "source": [
        "## Prompts\n",
        "\n",
        "Prompts are text used as instructions to your model. For more details have a look at the notebook [Intro to Prompt Design](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PrvHxWMidmTU",
        "outputId": "e86862d6-adf9-49c1-89ab-61f76ee5edd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'If today is Monday, then tomorrow must be **Tuesday**, not Wednesday.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Today is Monday, tomorrow is Wednesday.\n",
        "\n",
        "What is wrong with that statement?\n",
        "\"\"\"\n",
        "\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74988254"
      },
      "source": [
        "### **Prompt Template**\n",
        "\n",
        "[Prompt Template](https://python.langchain.com/v0.1/docs/modules/model_io/#prompt-templates) is an object that helps to create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
        "\n",
        "Think of it as an [`f-string`](https://realpython.com/python-f-strings/) in Python but for prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "abcc212d",
        "outputId": "a1676ca8-1d59-446b-fb7d-4b647d263d64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Prompt: \n",
            "I really want to travel to Rome. What should I do there?\n",
            "\n",
            "Respond in one short sentence\n",
            "\n",
            "-----------\n",
            "LLM Output: Immerse yourself in ancient history, marvel at priceless art, and savor delicious Italian cuisine.\n"
          ]
        }
      ],
      "source": [
        "# Notice \"location\" below, that is a placeholder for another value later\n",
        "template = \"\"\"\n",
        "I really want to travel to {location}. What should I do there?\n",
        "\n",
        "Respond in one short sentence\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"location\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location=\"Rome\")\n",
        "\n",
        "output = llm.invoke(final_prompt)\n",
        "\n",
        "print(f\"Final Prompt: {final_prompt}\")\n",
        "print(\"-----------\")\n",
        "print(f\"LLM Output: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed40bac2"
      },
      "source": [
        "### **Example Selectors**\n",
        "\n",
        "[Example selectors](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/) are an easy way to select from a series of examples to dynamically place in-context information into your prompt. Often used when the task is nuanced or has a large list of examples.\n",
        "\n",
        "Check out different types of example selectors [here](https://python.langchain.com/docs/how_to/example_selectors/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aaf36cd9"
      },
      "outputs": [],
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
        ")\n",
        "\n",
        "# Examples of locations that nouns are found\n",
        "examples = [\n",
        "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
        "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
        "    {\"input\": \"driver\", \"output\": \"car\"},\n",
        "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
        "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "12b4798b"
      },
      "outputs": [],
      "source": [
        "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    examples,\n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    embeddings,\n",
        "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    FAISS,\n",
        "    # This is the number of examples to produce.\n",
        "    k=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cf30107"
      },
      "outputs": [],
      "source": [
        "similar_prompt = FewShotPromptTemplate(\n",
        "    # The object that will help select examples\n",
        "    example_selector=example_selector,\n",
        "    # Your prompt\n",
        "    example_prompt=example_prompt,\n",
        "    # Customizations that will be added to the top and bottom of your prompt\n",
        "    prefix=\"Give the location an item is usually found in\",\n",
        "    suffix=\"Input: {noun}\\nOutput:\",\n",
        "    # What inputs your prompt will receive\n",
        "    input_variables=[\"noun\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "369442bb"
      },
      "outputs": [],
      "source": [
        "# Select a noun!\n",
        "my_noun = \"student\"\n",
        "\n",
        "print(similar_prompt.format(noun=my_noun))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bb910f2"
      },
      "outputs": [],
      "source": [
        "llm.invoke(similar_prompt.format(noun=my_noun))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8474c91d"
      },
      "source": [
        "### **Output Parsers**\n",
        "\n",
        "[Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/) help to format the output of a model. Usually used for structured output.\n",
        "\n",
        "Two main ideas:\n",
        "\n",
        "**1. Format Instructions**: An autogenerated prompt that tells the LLM how to format it's response based off desired result\n",
        "\n",
        "**2. Parser**: A method to extract model's text output into a desired structure (usually json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa59be3f"
      },
      "outputs": [],
      "source": [
        "# How you would like your response structured. This is basically a fancy prompt template\n",
        "response_schemas = [\n",
        "    ResponseSchema(\n",
        "        name=\"bad_string\", description=\"This a poorly formatted user input string\"\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"good_string\", description=\"This is your response, a reformatted response\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# How you would like to parse your output\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1079f0a"
      },
      "outputs": [],
      "source": [
        "# See the prompt template you created for formatting\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aaae5be"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You will be given a poorly formatted string from a user.\n",
        "Reformat it and make sure all the words are spelled correctly including country, city and state names\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "% USER INPUT:\n",
        "{user_input}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "prompt_value = prompt.format(user_input=\"welcom to dbln!\")\n",
        "\n",
        "print(prompt_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b116bb23"
      },
      "outputs": [],
      "source": [
        "llm_output = llm.invoke(prompt_value)\n",
        "llm_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "985aa814"
      },
      "outputs": [],
      "source": [
        "output_parser.parse(llm_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b43cec2"
      },
      "source": [
        "## Indexes\n",
        "\n",
        "[Indexes](https://docs.langchain.com/docs/components/indexing/) refer to ways to structure documents for LLMs to work with them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3f904e9"
      },
      "source": [
        "### **Document Loaders**\n",
        "\n",
        "Document loaders are ways to import data from other sources. See the [growing list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) of document loaders here. There are more on [LlamaIndex](https://llamahub.ai/) as well that work with LangChain Document Loaders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ee693520"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\"https://machinelearningcoban.com/2016/12/26/introduce/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "88d89ad7"
      },
      "outputs": [],
      "source": [
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "e814f930",
        "outputId": "13cf40ee-d86d-43e6-efaa-0f2fe3e5c160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 comments\n",
            "Here's a sample:\n",
            "\n",
            "[Document(metadata={'source': 'https://machinelearningcoban.com/2016/12/26/introduce/', 'title': 'Machine Learning cÆ¡ báº£n', 'language': 'No language found.'}, page_content='\\n\\n\\n\\n\\n\\nMachine Learning cÆ¡ báº£n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBÃ i 1: Giá»›i thiá»‡u vá» Machine Learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLatest\\nCon Ä‘Æ°á»ng há»c PhD cá»§a tÃ´i\\n37. TÃ­ch cháº­p hai chiá»u\\nDiá»…n Ä‘Ã n\\n36. Keras\\n35. LÆ°á»£c sá»­ Deep Learning\\nCon Ä‘Æ°á»ng há»c Khoa há»c dá»¯ liá»‡u cá»§a má»™t sinh viÃªn Kinh táº¿\\n34. Decision Trees (1): ID3\\n33. ÄÃ¡nh giÃ¡ há»‡ thá»‘ng phÃ¢n lá»›p\\nFundaML 3: CÃ¡c máº£ng ngáº«u nhiÃªn\\nFundaML 2: Ma tráº­n\\nFundaML 1: Máº£ng má»™t chiá»u\\nFundaML.com\\n32. Naive Bayes Classifier\\nViáº¿t vÃ  nháº­n xÃ©t cÃ¡c bÃ i bÃ¡o khoa há»c\\n31. Maximum Likelihood vÃ  Maximum A Posteriori\\nCon Ä‘Æ°á»ng há»c ToÃ¡n cá»§a tÃ´i\\n30. Ã”n táº­p XÃ¡c Suáº¥t\\nQ2. Transfer Learning\\n29. Linear Discriminant Analysis\\nQ1. Quick Notes 1\\n28. Principal Component Analysis (2/2)\\n27. Principal Component Analysis (1/2)\\n26. Singular Value Decomposition\\n25. Matrix Factorization Collaborative Filtering\\n24. Neighborhood-Based Collaborative Filtering\\n23. Content-based Recommendation Systems\\n22. Multi-class SVM\\n21. Kernel SVM\\n20. Soft Margin SVM\\n19. Support Vector Machine\\n18. Duality\\n17. Convex Optimization Problems\\n16. Convex sets vÃ  convex functions\\n15. Overfitting\\n14. Multi-layer Perceptron vÃ  Backpropagation\\n13. Softmax Regression\\n12. Binary Classifiers\\n11. Feature Engineering\\n\\n10. Logistic Regression\\n9. Perceptron Learning Algorithm\\n8. Gradient Descent (2/2)\\n7. Gradient Descent (1/2)\\n6. K-nearest neighbors\\n5. K-means Clustering - Applications\\n4. K-means Clustering\\n3. Linear Regression\\n2. PhÃ¢n nhÃ³m cÃ¡c thuáº­t toÃ¡n Machine Learning\\n1. Giá»›i thiá»‡u vá» Machine Learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMachine Learning cÆ¡ báº£n\\n\\n\\n\\n\\n About\\nIndex\\nTags\\nCategories\\nArchive\\nMath\\n\\nCopyrights\\n\\nebook\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBÃ i 2: PhÃ¢n nhÃ³m cÃ¡c thuáº­t toÃ¡n Machine Learning Â»\\n\\n\\n\\n\\nBÃ i 1: Giá»›i thiá»‡u vá» Machine Learning\\n\\nGeneral\\n\\nDec 26, 2016\\n            \\n\\n\\n\\n\\nNhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, AI - Artificial Intelligence (TrÃ­ Tuá»‡ NhÃ¢n Táº¡o), vÃ  cá»¥ thá»ƒ hÆ¡n lÃ  Machine Learning (Há»c MÃ¡y hoáº·c MÃ¡y Há»c) ná»•i lÃªn nhÆ° má»™t báº±ng chá»©ng cá»§a cuá»™c cÃ¡ch máº¡ng cÃ´ng nghiá»‡p láº§n thá»© tÆ° (1 - Ä‘á»™ng cÆ¡ hÆ¡i nÆ°á»›c, 2 - nÄƒng lÆ°á»£ng Ä‘iá»‡n, 3 - cÃ´ng nghá»‡ thÃ´ng tin). TrÃ­ Tuá»‡ NhÃ¢n Táº¡o Ä‘ang len lá»i vÃ o má»i lÄ©nh vá»±c trong Ä‘á»i sá»‘ng mÃ  cÃ³ thá»ƒ chÃºng ta khÃ´ng nháº­n ra. Xe tá»± hÃ nh cá»§a Google vÃ  Tesla, há»‡ thá»‘ng tá»± tag khuÃ´n máº·t trong áº£nh cá»§a Facebook, trá»£ lÃ½ áº£o Siri cá»§a Apple, há»‡ thá»‘ng gá»£i Ã½ sáº£n pháº©m cá»§a Amazon, há»‡ thá»‘ng gá»£i Ã½ phim cá»§a Netflix, mÃ¡y chÆ¡i cá» vÃ¢y AlphaGo cá»§a Google DeepMind, â€¦, chá»‰ lÃ  má»™t vÃ i trong vÃ´ vÃ n nhá»¯ng á»©ng dá»¥ng cá»§a AI/Machine Learning. (Xem thÃªm Jarvis - trá»£ lÃ½ thÃ´ng minh cho cÄƒn nhÃ  cá»§a Mark Zuckerberg)\\nMachine Learning lÃ  má»™t táº­p con cá»§a AI. Theo Ä‘á»‹nh nghÄ©a cá»§a Wikipedia, Machine learning is the subfield of computer science that â€œgives computers the ability to learn without being explicitly programmedâ€. NÃ³i Ä‘Æ¡n giáº£n, Machine Learning lÃ  má»™t lÄ©nh vá»±c nhá» cá»§a Khoa Há»c MÃ¡y TÃ­nh, nÃ³ cÃ³ kháº£ nÄƒng tá»± há»c há»i dá»±a trÃªn dá»¯ liá»‡u Ä‘Æ°a vÃ o mÃ  khÃ´ng cáº§n pháº£i Ä‘Æ°á»£c láº­p trÃ¬nh cá»¥ thá»ƒ. Báº¡n Nguyá»…n XuÃ¢n KhÃ¡nh táº¡i Ä‘áº¡i há»c Maryland Ä‘ang viáº¿t má»™t cuá»‘n sÃ¡ch vá» Machine Learning báº±ng tiáº¿ng Viá»‡t khÃ¡ thÃº vá»‹, cÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o bÃ i Machine Learning lÃ  gÃ¬?.\\nNhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, khi mÃ  kháº£ nÄƒng tÃ­nh toÃ¡n cá»§a cÃ¡c mÃ¡y tÃ­nh Ä‘Æ°á»£c nÃ¢ng lÃªn má»™t táº§m cao má»›i vÃ  lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“ Ä‘Æ°á»£c thu tháº­p bá»Ÿi cÃ¡c hÃ£ng cÃ´ng nghá»‡ lá»›n, Machine Learning Ä‘Ã£ tiáº¿n thÃªm má»™t bÆ°á»›c dÃ i vÃ  má»™t lÄ©nh vá»±c má»›i Ä‘Æ°á»£c ra Ä‘á»i gá»i lÃ  Deep Learning (Há»c SÃ¢u - thá»±c sá»± tÃ´i khÃ´ng muá»‘n dá»‹ch tá»« nÃ y ra tiáº¿ng Viá»‡t). Deep Learning Ä‘Ã£ giÃºp mÃ¡y tÃ­nh thá»±c thi nhá»¯ng viá»‡c tÆ°á»Ÿng chá»«ng nhÆ° khÃ´ng thá»ƒ vÃ o 10 nÄƒm trÆ°á»›c: phÃ¢n loáº¡i cáº£ ngÃ n váº­t thá»ƒ khÃ¡c nhau trong cÃ¡c bá»©c áº£nh, tá»± táº¡o chÃº thÃ­ch cho áº£nh, báº¯t chÆ°á»›c giá»ng nÃ³i vÃ  chá»¯ viáº¿t cá»§a con ngÆ°á»i, giao tiáº¿p vá»›i con ngÆ°á»i, hay tháº­m chÃ­ cáº£ sÃ¡ng tÃ¡c vÄƒn hay Ã¢m nháº¡c (Xem thÃªm 8 Inspirational Applications of Deep Learning)\\n\\n\\n\\n\\n\\nMá»‘i quan há»‡ giá»¯a AI, Machine Learning vÃ  Deep Learning.  (Nguá»“n: Whatâ€™s the Difference Between Artificial Intelligence, Machine Learning, and Deep Learning?)\\n\\n\\nMá»¥c Ä‘Ã­ch viáº¿t Blog\\nNhu cáº§u vá» nhÃ¢n lá»±c ngÃ nh Machine Learning (Deep Learning) Ä‘ang ngÃ y má»™t cao, kÃ©o theo Ä‘Ã³ nhu cáº§u há»c Machine Learning trÃªn tháº¿ giá»›i vÃ  á»Ÿ Viá»‡t Nam ngÃ y má»™t lá»›n. CÃ¡ nhÃ¢n tÃ´i cÅ©ng muá»‘n há»‡ thá»‘ng láº¡i kiáº¿n thá»©c cá»§a mÃ¬nh vá» lÄ©nh vá»±c nÃ y Ä‘á»ƒ chuáº©n bá»‹ cho tÆ°Æ¡ng lai (Ä‘Ã¢y lÃ  má»™t trong nhá»¯ng má»¥c tiÃªu cá»§a tÃ´i trong nÄƒm 2017). TÃ´i sáº½ cá»‘ gáº¯ng Ä‘i tá»« nhá»¯ng thuáº­t toÃ¡n cÆ¡ báº£n nháº¥t cá»§a Machine Learning kÃ¨m theo cÃ¡c vÃ­ dá»¥ vÃ  mÃ£ nguá»“n trong má»—i bÃ i viáº¿t. TÃ´i sáº½ viáº¿t 1-2 tuáº§n 1 bÃ i (viá»‡c viáº¿t cÃ¡c cÃ´ng thá»©c toÃ¡n vÃ  code trÃªn blog thá»±c sá»± tá»‘n nhiá»u thá»i gian hÆ¡n tÃ´i tá»«ng nghÄ©). Äá»“ng thá»i, tÃ´i cÅ©ng mong muá»‘n nháº­n Ä‘Æ°á»£c pháº£n há»“i cá»§a báº¡n Ä‘á»c Ä‘á»ƒ qua nhá»¯ng tháº£o luáº­n, tÃ´i vÃ  cÃ¡c báº¡n cÃ³ thá»ƒ náº¯m báº¯t Ä‘Æ°á»£c cÃ¡c thuáº­t toÃ¡n nÃ y.\\nVá»›i nhá»¯ng tá»« chuyÃªn ngÃ nh, tÃ´i sáº½ dÃ¹ng song song cáº£ tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t, tuy nhiÃªn sáº½ Æ°u tiÃªn tiáº¿ng Anh vÃ¬ thuáº­n tiá»‡n hÆ¡n cho cÃ¡c báº¡n  trong viá»‡c tra cá»©u cÃ¡c tÃ i liá»‡u tiáº¿ng Anh.\\nKhi chuáº©n bá»‹ cÃ¡c bÃ i viáº¿t, tÃ´i sáº½ giáº£ Ä‘á»‹nh ráº±ng báº¡n Ä‘á»c cÃ³ má»™t chÃºt kiáº¿n thá»©c vá» Äáº¡i Sá»‘ Tuyáº¿n TÃ­nh (Linear Algebra), XÃ¡c Suáº¥t Thá»‘ng KÃª (Probability and Statistics) vÃ  cÃ³ kinh nghiá»‡m vá» láº­p trÃ¬nh Python. Náº¿u báº¡n chÆ°a cÃ³ nhiá»u kinh nghiá»‡m vá» cÃ¡c lÄ©nh vá»±c nÃ y, Ä‘á»«ng quÃ¡ lo láº¯ng vÃ¬ má»—i bÃ i sáº½ chá»‰ sá»­ dá»¥ng má»™t vÃ i ká»¹ thuáº­t cÆ¡ báº£n. HÃ£y Ä‘á»ƒ láº¡i cÃ¢u há»i cá»§a báº¡n á»Ÿ pháº§n Comment bÃªn dÆ°á»›i má»—i bÃ i, tÃ´i sáº½ tháº£o luáº­n thÃªm vá»›i cÃ¡c báº¡n.\\n\\nTrong bÃ i tiáº¿p theo cá»§a blog nÃ y, tÃ´i sáº½ giá»›i thiá»‡u vá» cÃ¡c nhÃ³m thuáº­t toÃ¡n Machine learning cÆ¡ báº£n. Má»i cÃ¡c báº¡n theo dÃµi.\\nTham kháº£o thÃªm\\nCÃ¡c khÃ³a há»c\\nTiáº¿ng Anh\\n\\nMachine Learning vá»›i tháº§y Andrew Ng trÃªn Coursera (KhÃ³a há»c ná»•i tiáº¿ng nháº¥t vá» Machine Learning)\\nDeep Learning by Google trÃªn Udacity (KhÃ³a há»c nÃ¢ng cao hÆ¡n vá» Deep Learning vá»›i Tensorflow)\\nMachine Learning mastery (CÃ¡c thuáº­t toÃ¡n Machine Learning cÆ¡ báº£n)\\n\\n\\nCÃ¡c trang Machine Learning tiáº¿ng Viá»‡t\\n\\nMachine Learning trong Xá»­ LÃ½ NgÃ´n Ngá»¯ Tá»± NhiÃªn - NhÃ³m ÄÃ´ng Du Nháº­t Báº£n\\nMachine Learning cho ngÆ°á»i má»›i báº¯t Ä‘áº§u - Ã”ng XuÃ¢n Há»“ng JAIST.\\nMachine Learning book for Vietnamese - Nguyá»…n XuÃ¢n KhÃ¡nh University of Maryland\\n\\n\\n\\nNáº¿u cÃ³ cÃ¢u há»i, Báº¡n cÃ³ thá»ƒ Ä‘á»ƒ láº¡i comment bÃªn dÆ°á»›i hoáº·c trÃªn Forum Ä‘á»ƒ nháº­n Ä‘Æ°á»£c cÃ¢u tráº£ lá»i sá»›m hÆ¡n.\\n\\nBáº¡n Ä‘á»c cÃ³ thá»ƒ á»§ng há»™ blog qua \\'Buy me a cofee\\' á»Ÿ gÃ³c trÃªn bÃªn trÃ¡i cá»§a blog.\\n\\n\\nTÃ´i vá»«a hoÃ n thÃ nh cuá»‘n ebook \\'Machine Learning cÆ¡ báº£n\\', báº¡n cÃ³ thá»ƒ Ä‘áº·t sÃ¡ch táº¡i Ä‘Ã¢y.\\n\\nCáº£m Æ¡n báº¡n.\\n\\n\\n\\nBÃ i 2: PhÃ¢n nhÃ³m cÃ¡c thuáº­t toÃ¡n Machine Learning Â»\\n\\n\\n\\n\\n\\nPlease enable JavaScript to view the comments powered by Disqus.\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n\\nShare\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDiá»…n Ä‘Ã n\\n\\n \\n\\n\\nInteractive Learning\\n\\n \\n\\n\\nFacebook page\\n\\n\\nMachine Learning cÆ¡ báº£n\\n\\n\\n\\nFacebook group\\n\\n \\n\\n\\nRecommended books\\n\\n \"Pattern recognition and Machine Learning.\", C. Bishop \\n \"The Elements of Statistical Learning\", T. Hastie et al.  \\n \"Computer Vision:  Models, Learning, and Inference\", Simon J.D. Prince \\n \"Convex Optimization\", Boyd and Vandenberghe\\n\\n\\n\\nRecommended courses\\n\\n \"Machine Learning\", Andrew Ng \\n CS224n: Natural Language Processing with Deep Learning\\n CS231n: Convolutional Neural Networks for Visual Recognition\\n CS246: Mining Massive Data Sets\\n CS20SI: Tensorflow for Deep Learning Research \\n Introduction to Computer Science and Programming Using Python\\n\\n\\n\\nOthers\\n\\n Top-down learning path: Machine Learning for Software Engineers\\n Blog nÃ y Ä‘Æ°á»£c táº¡o nhÆ° tháº¿ nÃ o?\\n ChÃºng tÃ´i Ä‘Ã£ apply vÃ  há»c tiáº¿n sá»¹ nhÆ° tháº¿ nÃ o? (1/2)\\n ChÃºng tÃ´i Ä‘Ã£ apply vÃ  há»c tiáº¿n sá»¹ nhÆ° tháº¿ nÃ o? (2/2)\\n 8 Inspirational Applications of Deep Learning\\n Matrix calculus\\n TensorFlow-Examples\\n Eight Easy Steps To Get Started Learning Artificial Intelligence\\n The 9 Deep Learning Papers You Need To Know About\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Found {len(data)} comments\")\n",
        "print(f\"Here's a sample:\\n\\n{data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e9601db"
      },
      "source": [
        "### **Text Splitters**\n",
        "\n",
        "[Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) are a way to deal with input token limits of LLMs by splitting text into chunks.\n",
        "\n",
        "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/docs/modules/data_connection/document_transformers/) to see which is best for your use case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CbA6yXonidz9"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\"https://machinelearningcoban.com/2016/12/26/introduce/\")\n",
        "pg_work = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "d19acb18"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(pg_work)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "e3090f05",
        "outputId": "1af4f985-7fcf-4c34-9dab-51ab1e50b381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 13 documents\n"
          ]
        }
      ],
      "source": [
        "print(f\"You have {len(texts)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "87a0f45a",
        "outputId": "d964542b-1ad1-4fbb-9ef9-0f859e689260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preview:\n",
            "Machine Learning lÃ  má»™t táº­p con cá»§a AI. Theo Ä‘á»‹nh nghÄ©a cá»§a Wikipedia, Machine learning is the subfield of computer science that â€œgives computers the ability to learn without being explicitly programmedâ€. NÃ³i Ä‘Æ¡n giáº£n, Machine Learning lÃ  má»™t lÄ©nh vá»±c nhá» cá»§a Khoa Há»c MÃ¡y TÃ­nh, nÃ³ cÃ³ kháº£ nÄƒng tá»± há»c há»i dá»±a trÃªn dá»¯ liá»‡u Ä‘Æ°a vÃ o mÃ  khÃ´ng cáº§n pháº£i Ä‘Æ°á»£c láº­p trÃ¬nh cá»¥ thá»ƒ. Báº¡n Nguyá»…n XuÃ¢n KhÃ¡nh táº¡i Ä‘áº¡i há»c Maryland Ä‘ang viáº¿t má»™t cuá»‘n sÃ¡ch vá» Machine Learning báº±ng tiáº¿ng Viá»‡t khÃ¡ thÃº vá»‹, cÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o bÃ i Machine Learning lÃ  gÃ¬?. \n",
            "\n",
            "Nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, khi mÃ  kháº£ nÄƒng tÃ­nh toÃ¡n cá»§a cÃ¡c mÃ¡y tÃ­nh Ä‘Æ°á»£c nÃ¢ng lÃªn má»™t táº§m cao má»›i vÃ  lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“ Ä‘Æ°á»£c thu tháº­p bá»Ÿi cÃ¡c hÃ£ng cÃ´ng nghá»‡ lá»›n, Machine Learning Ä‘Ã£ tiáº¿n thÃªm má»™t bÆ°á»›c dÃ i vÃ  má»™t lÄ©nh vá»±c má»›i Ä‘Æ°á»£c ra Ä‘á»i gá»i lÃ  Deep Learning (Há»c SÃ¢u - thá»±c sá»± tÃ´i khÃ´ng muá»‘n dá»‹ch tá»« nÃ y ra tiáº¿ng Viá»‡t). Deep Learning Ä‘Ã£ giÃºp mÃ¡y tÃ­nh thá»±c thi nhá»¯ng viá»‡c tÆ°á»Ÿng chá»«ng nhÆ° khÃ´ng thá»ƒ vÃ o 10 nÄƒm trÆ°á»›c: phÃ¢n loáº¡i cáº£ ngÃ n váº­t thá»ƒ khÃ¡c nhau trong cÃ¡c bá»©c áº£nh, tá»± táº¡o chÃº thÃ­ch cho áº£nh, báº¯t chÆ°á»›c giá»ng nÃ³i vÃ  chá»¯ viáº¿t cá»§a con ngÆ°á»i, giao tiáº¿p vá»›i con ngÆ°á»i, hay tháº­m chÃ­ cáº£ sÃ¡ng tÃ¡c vÄƒn hay Ã¢m nháº¡c (Xem thÃªm 8 Inspirational Applications of Deep Learning)\n"
          ]
        }
      ],
      "source": [
        "print(\"Preview:\")\n",
        "print(texts[5].page_content, \"\\n\")\n",
        "print(texts[6].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f85defb"
      },
      "source": [
        "### **Retrievers**\n",
        "\n",
        "[Retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers) are a way of storing data such that it can be queried by a language model. Easy way to combine documents with language models.\n",
        "\n",
        "There are [many different types of retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers.html#advanced-retrieval-types), the most widely supported is the `VectorStoreRetriever`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Vu_uBr4rKHvP"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\"https://machinelearningcoban.com/2016/12/27/categories/\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnifR4tZkV5P"
      },
      "source": [
        "Here we use [Facebook AI Similarity Search (FAISS)](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/), a library and a vector database for similarity search and clustering of dense vectors. To generate dense vectors, a.k.a. embeddings, we use [LangChain text embeddings model with Vertex AI Embeddings for Text](https://python.langchain.com/docs/integrations/text_embedding/google_vertex_ai_palm) .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1dab1c20"
      },
      "outputs": [],
      "source": [
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Embed your texts\n",
        "db = FAISS.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "e62372be",
        "outputId": "6a063d89-62f1-4796-8cdb-d4e0e10135d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'VertexAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7936875729f0>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Init your retriever. Asking for just 1 document back\n",
        "retriever = db.as_retriever()\n",
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "3846a3b5",
        "outputId": "b6db6850-a07f-4372-cb43-96cc0ea59a3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id='9968eefa-58d6-4a8c-a7e4-190ecb1a5e1e', metadata={'source': 'https://machinelearningcoban.com/2016/12/27/categories/', 'title': 'Machine Learning cÆ¡ báº£n', 'language': 'No language found.'}, page_content='3. TÃ i liá»‡u tham kháº£o\\n\\n\\n\\n\\n1. PhÃ¢n nhÃ³m dá»±a trÃªn phÆ°Æ¡ng thá»©c há»c\\nTheo phÆ°Æ¡ng thá»©c há»c, cÃ¡c thuáº­t toÃ¡n Machine Learning thÆ°á»ng Ä‘Æ°á»£c chia lÃ m 4 nhÃ³m: Supervised learning, Unsupervised learning, Semi-supervised learning vÃ  Reinforcement learning. CÃ³ má»™t sá»‘ cÃ¡ch phÃ¢n nhÃ³m khÃ´ng cÃ³ Semi-supervised learning hoáº·c Reinforcement learning.'), Document(id='491b0365-e971-4a97-bd7a-a7d4d9a86889', metadata={'source': 'https://machinelearningcoban.com/2016/12/27/categories/', 'title': 'Machine Learning cÆ¡ báº£n', 'language': 'No language found.'}, page_content='10. Logistic Regression\\n9. Perceptron Learning Algorithm\\n8. Gradient Descent (2/2)\\n7. Gradient Descent (1/2)\\n6. K-nearest neighbors\\n5. K-means Clustering - Applications\\n4. K-means Clustering\\n3. Linear Regression\\n2. PhÃ¢n nhÃ³m cÃ¡c thuáº­t toÃ¡n Machine Learning\\n1. Giá»›i thiá»‡u vá» Machine Learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMachine Learning cÆ¡ báº£n\\n\\n\\n\\n\\n About\\nIndex\\nTags\\nCategories\\nArchive\\nMath\\n\\nCopyrights\\n\\nebook\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nÂ« BÃ i 1: Giá»›i thiá»‡u vá» Machine Learning\\n\\nBÃ i 3: Linear Regression Â»\\n\\n\\n\\n\\nBÃ i 2: PhÃ¢n nhÃ³m cÃ¡c thuáº­t toÃ¡n Machine Learning\\n\\nGeneral\\n\\nDec 27, 2016\\n            \\n\\n\\n\\n\\nCÃ³ hai cÃ¡ch phá»• biáº¿n phÃ¢n nhÃ³m cÃ¡c thuáº­t toÃ¡n Machine learning. Má»™t lÃ  dá»±a trÃªn phÆ°Æ¡ng thá»©c há»c (learning style), hai lÃ  dá»±a trÃªn chá»©c nÄƒng (function) (cá»§a má»—i thuáº­t toÃ¡n).\\nTrong trang nÃ y:\\n\\n\\n1. PhÃ¢n nhÃ³m dá»±a trÃªn phÆ°Æ¡ng thá»©c há»c\\n\\nSupervised Learning (Há»c cÃ³ giÃ¡m sÃ¡t)\\n\\nClassification (PhÃ¢n loáº¡i)\\nRegression (Há»“i quy)\\n\\n\\nUnsupervised Learning (Há»c khÃ´ng giÃ¡m sÃ¡t)\\n\\nClustering (phÃ¢n nhÃ³m)\\nAssociation\\n\\n\\nSemi-Supervised Learning (Há»c bÃ¡n giÃ¡m sÃ¡t)\\nReinforcement Learning (Há»c Cá»§ng Cá»‘)\\n\\n\\n2. PhÃ¢n nhÃ³m dá»±a trÃªn chá»©c nÄƒng\\n\\nRegression Algorithms\\nClassification Algorithms\\nInstance-based Algorithms\\nRegularization Algorithms\\nBayesian Algorithms\\nClustering Algorithms\\nArtificial Neural Network Algorithms\\nDimensionality Reduction Algorithms\\nEnsemble Algorithms\\n\\n\\n3. TÃ i liá»‡u tham kháº£o'), Document(id='8c4e681e-5615-4da0-9236-562d0f9f2bd2', metadata={'source': 'https://machinelearningcoban.com/2016/12/27/categories/', 'title': 'Machine Learning cÆ¡ báº£n', 'language': 'No language found.'}, page_content='Machine Learning cÆ¡ báº£n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBÃ i 2: PhÃ¢n nhÃ³m cÃ¡c thuáº­t toÃ¡n Machine Learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLatest\\nCon Ä‘Æ°á»ng há»c PhD cá»§a tÃ´i\\n37. TÃ­ch cháº­p hai chiá»u\\nDiá»…n Ä‘Ã n\\n36. Keras\\n35. LÆ°á»£c sá»­ Deep Learning\\nCon Ä‘Æ°á»ng há»c Khoa há»c dá»¯ liá»‡u cá»§a má»™t sinh viÃªn Kinh táº¿\\n34. Decision Trees (1): ID3\\n33. ÄÃ¡nh giÃ¡ há»‡ thá»‘ng phÃ¢n lá»›p\\nFundaML 3: CÃ¡c máº£ng ngáº«u nhiÃªn\\nFundaML 2: Ma tráº­n\\nFundaML 1: Máº£ng má»™t chiá»u\\nFundaML.com\\n32. Naive Bayes Classifier\\nViáº¿t vÃ  nháº­n xÃ©t cÃ¡c bÃ i bÃ¡o khoa há»c\\n31. Maximum Likelihood vÃ  Maximum A Posteriori\\nCon Ä‘Æ°á»ng há»c ToÃ¡n cá»§a tÃ´i\\n30. Ã”n táº­p XÃ¡c Suáº¥t\\nQ2. Transfer Learning\\n29. Linear Discriminant Analysis\\nQ1. Quick Notes 1\\n28. Principal Component Analysis (2/2)\\n27. Principal Component Analysis (1/2)\\n26. Singular Value Decomposition\\n25. Matrix Factorization Collaborative Filtering\\n24. Neighborhood-Based Collaborative Filtering\\n23. Content-based Recommendation Systems\\n22. Multi-class SVM\\n21. Kernel SVM\\n20. Soft Margin SVM\\n19. Support Vector Machine\\n18. Duality\\n17. Convex Optimization Problems\\n16. Convex sets vÃ  convex functions\\n15. Overfitting\\n14. Multi-layer Perceptron vÃ  Backpropagation\\n13. Softmax Regression\\n12. Binary Classifiers\\n11. Feature Engineering'), Document(id='62655b88-d37b-4d9b-b275-90e9c51784a4', metadata={'source': 'https://machinelearningcoban.com/2016/12/27/categories/', 'title': 'Machine Learning cÆ¡ báº£n', 'language': 'No language found.'}, page_content='Huáº¥n luyá»‡n cho mÃ¡y tÃ­nh chÆ¡i game Mario\\n\\n\\n\\n2. PhÃ¢n nhÃ³m dá»±a trÃªn chá»©c nÄƒng\\nCÃ³ má»™t cÃ¡ch phÃ¢n nhÃ³m thá»© hai dá»±a trÃªn chá»©c nÄƒng cá»§a cÃ¡c thuáº­t toÃ¡n. Trong pháº§n nÃ y, tÃ´i xin chá»‰ liá»‡t kÃª cÃ¡c thuáº­t toÃ¡n. ThÃ´ng tin cá»¥ thá»ƒ sáº½ Ä‘Æ°á»£c trÃ¬nh bÃ y trong cÃ¡c bÃ i viáº¿t khÃ¡c táº¡i blog nÃ y. Trong quÃ¡ trÃ¬nh viáº¿t, tÃ´i cÃ³ thá»ƒ sáº½ thÃªm bá»›t má»™t sá»‘ thuáº­t toÃ¡n.\\n\\n\\nRegression Algorithms\\n\\nLinear Regression\\nLogistic Regression\\nStepwise Regression\\n\\n\\n\\nClassification Algorithms\\n\\nLinear Classifier\\nSupport Vector Machine (SVM)\\nKernel SVM\\nSparse Representation-based classification (SRC)\\n\\n\\n\\nInstance-based Algorithms\\n\\nk-Nearest Neighbor (kNN)\\nLearning Vector Quantization (LVQ)\\n\\n\\n\\nRegularization Algorithms\\n\\nRidge Regression\\nLeast Absolute Shrinkage and Selection Operator (LASSO)\\nLeast-Angle Regression (LARS)\\n\\n\\n\\nBayesian Algorithms\\n\\nNaive Bayes\\nGaussian Naive Bayes\\n\\n\\n\\nClustering Algorithms\\n\\nk-Means clustering\\nk-Medians\\nExpectation Maximization (EM)\\n\\n\\n\\nArtificial Neural Network Algorithms\\n\\nPerceptron\\nSoftmax Regression\\nMulti-layer Perceptron\\nBack-Propagation \\n\\n\\n\\nDimensionality Reduction Algorithms\\n\\nPrincipal Component Analysis (PCA)\\nLinear Discriminant Analysis (LDA)\\n\\n\\n\\nEnsemble Algorithms\\n\\nBoosting\\nAdaBoost\\nRandom Forest\\n\\nVÃ  cÃ²n ráº¥t nhiá»u cÃ¡c thuáº­t toÃ¡n khÃ¡c.\\n\\n\\n3. TÃ i liá»‡u tham kháº£o\\n\\n\\nA Tour of Machine Learning Algorithms\\n\\n\\nÄiá»ƒm qua cÃ¡c thuáº­t toÃ¡n Machine Learning hiá»‡n Ä‘áº¡i\\n\\n\\n\\n\\nNáº¿u cÃ³ cÃ¢u há»i, Báº¡n cÃ³ thá»ƒ Ä‘á»ƒ láº¡i comment bÃªn dÆ°á»›i hoáº·c trÃªn Forum Ä‘á»ƒ nháº­n Ä‘Æ°á»£c cÃ¢u tráº£ lá»i sá»›m hÆ¡n.')]\n"
          ]
        }
      ],
      "source": [
        "docs = retriever.get_relevant_documents(\n",
        "    \"cÃ³ bao nhiÃªu loáº¡i machine learning\"\n",
        ")\n",
        "\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24193139"
      },
      "source": [
        "### Vector Stores\n",
        "\n",
        "[Vector Store](https://python.langchain.com/docs/modules/data_connection/vectorstores) is a common type of index or a database to store vectors (numerical embeddings). Conceptually, think of them as tables with a column for embeddings (vectors) and a column for metadata.\n",
        "\n",
        "Example\n",
        "\n",
        "| Embedding                                             | Metadata           |\n",
        "| ----------------------------------------------------- | ------------------ |\n",
        "| `[-0.00015641732898075134, -0.003165106289088726, ...]` | `{'date' : '1/2/23}` |\n",
        "| `[-0.00035465431654651654, 1.4654131651654516546, ...]` | `{'date' : '1/3/23}` |\n",
        "\n",
        "- [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) are easy to work with locally.\n",
        "- [Vertex AI Vector Search (Matching Engine)](https://cloud.google.com/blog/products/ai-machine-learning/vertex-matching-engine-blazing-fast-and-massively-scalable-nearest-neighbor-search) is fully managed vector store on Google Cloud, developers can just add the embeddings to its index and issue a search query with a key embedding for the blazingly fast vector search.\n",
        "\n",
        "<br/>\n",
        "\n",
        "LangChain VectorStore is [integrated with Vertex AI Vector Search](https://python.langchain.com/v0.2/docs/integrations/vectorstores/google_vertex_ai_vector_search/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c5533ad"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\"http://www.paulgraham.com/worked.html\")\n",
        "documents = loader.load()\n",
        "\n",
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "661fdf19"
      },
      "outputs": [],
      "source": [
        "print(f\"You have {len(texts)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e99ac0ea"
      },
      "outputs": [],
      "source": [
        "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89e7758c"
      },
      "outputs": [],
      "source": [
        "print(f\"You have {len(embedding_list)} embeddings\")\n",
        "print(f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ac358c5"
      },
      "source": [
        "VectorStore stores your embeddings (â˜ï¸) and makes them easily searchable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9b9b79b"
      },
      "source": [
        "## Memory\n",
        "\n",
        "[Memory](https://python.langchain.com/docs/modules/memory/) is the concept of storing and retrieving data in the process of a conversation. Memory helps LLMs remember information you've chatted about in the past or more complicated information retrieval.\n",
        "\n",
        "There are many types of memory, explore [the documentation](https://python.langchain.com/docs/modules/memory/) to see which one fits your use case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f43b49da"
      },
      "source": [
        "### ConversationBufferMemory\n",
        "\n",
        "Memory keeps conversation state throughout a user's interactions with a language model. `ConversationBufferMemory` memory allows for storing of messages and then extracts the messages in a variable.\n",
        "\n",
        "We'll use `ConversationChain` to have a conversation and load context from memory. We will look into Chains in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "893a18c1"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG0y5VXL8R6Y"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7TfAEQDoPK_"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What are some popular places I can see in France?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoJ9HnR6oKbT"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What question did I ask first?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f29fc79c"
      },
      "source": [
        "## Chains â›“ï¸â›“ï¸â›“ï¸\n",
        "\n",
        "Chains are a generic concept in LangChain allowing to combine different LLM calls and action automatically.\n",
        "\n",
        "Ex:\n",
        "\n",
        "```\n",
        "Summary #1, Summary #2, Summary #3 --> Final Summary\n",
        "```\n",
        "\n",
        "There are [many applications of chains](https://python.langchain.com/docs/modules/chains) search to see which are best for your use case.\n",
        "\n",
        "We'll cover a few of them:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c34ba415"
      },
      "source": [
        "### 1. Simple Sequential Chains\n",
        "\n",
        "[Sequential chains](https://python.langchain.com/en/latest/modules/chains/generic/sequential_chains.html) are a series of chains, called in deterministic order. `SimpleSequentialChain` are easy chains where each step uses the output of an LLM as an input into another. Good for breaking up tasks (and keeping the LLM focused).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43d4494a"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
        "% USER LOCATION\n",
        "{user_location}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
        "\n",
        "# Holds my 'location' chain\n",
        "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6c8e00f"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
        "% MEAL\n",
        "{user_meal}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
        "\n",
        "# Holds my 'meal' chain\n",
        "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e0b83f2"
      },
      "outputs": [],
      "source": [
        "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d19c64d"
      },
      "outputs": [],
      "source": [
        "review = overall_chain.invoke(\"Rome\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6191bf5"
      },
      "source": [
        "### 2. Summarization Chain\n",
        "\n",
        "[Summarization Chain](https://python.langchain.com/docs/modules/chains/popular/summarize) easily runs through a long numerous documents and get a summary.\n",
        "\n",
        "There are multiple chain types such as Stuffing, Map-Reduce, Refine, Map-Rerank. Check out [documentation](https://python.langchain.com/docs/modules/chains/how_to/) for other chain types besides `map-reduce`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f218c3e"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\n",
        "    \"https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-with-text-embeddings\"\n",
        ")\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"# of words in the document = {len(documents[0].page_content)}\")\n",
        "\n",
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
        "chain.run(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta-Vt4t3wTQ7"
      },
      "source": [
        "### 3. Question/Answering Chain\n",
        "\n",
        "[Question Answering Chains](https://python.langchain.com/v0.1/docs/use_cases/question_answering/) easily do QA over a set of documents using QA chain. There are multiple ways to do this with LangChain. We use [**RetrievalQA** chain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db) which uses `load_qa_chain` under the hood.\n",
        "\n",
        "![QA Process](https://miro.medium.com/v2/resize:fit:2000/format:webp/0*x2f4Es8-NO6zUmks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QjgeWSMw0Bb"
      },
      "outputs": [],
      "source": [
        "# Load GOOG's 10K annual report (92 pages).\n",
        "url = \"https://abc.xyz/assets/investor/static/pdf/20230203_alphabet_10K.pdf\"\n",
        "loader = PyPDFLoader(url)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJecKul0xgWT"
      },
      "outputs": [],
      "source": [
        "# split the documents into chunks\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(f\"# of documents = {len(docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V8AoFiQyMSx"
      },
      "outputs": [],
      "source": [
        "# select embedding engine - we use Vertex AI Embeddings API\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st9_gVXmyGf6"
      },
      "outputs": [],
      "source": [
        "# Store docs in local VectorStore as index\n",
        "# it may take a while since API is rate limited\n",
        "db = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29At6L691XZr"
      },
      "outputs": [],
      "source": [
        "# Expose index to the retriever\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibQdjXjw1foF"
      },
      "outputs": [],
      "source": [
        "# Create chain to answer questions\n",
        "\n",
        "# Uses LLM to synthesize results from the search index.\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvjxHX6a105H"
      },
      "outputs": [],
      "source": [
        "query = \"What was Alphabet's net income in 2022?\"\n",
        "result = qa.invoke({\"query\": query})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLQPrNUq2aiF"
      },
      "source": [
        "![executive-overview](https://storage.googleapis.com/github-repo/generative-ai/gemini/orchestration/intro_langchain_gemini/executive-overview.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of9XWLw9C653"
      },
      "outputs": [],
      "source": [
        "query = \"How much office space reduction took place in 2023?\"\n",
        "result = qa({\"query\": query})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb9RHMw5DKjQ"
      },
      "source": [
        "![office-space-reduction](https://storage.googleapis.com/github-repo/generative-ai/gemini/orchestration/intro_langchain_gemini/office-space-reduction.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_langchain_gemini.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}